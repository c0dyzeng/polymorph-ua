{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import CategoricalEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import clone\n",
    "#CategoricalEncoder is part of sklearn's developer version, which you can't just update with conda. If you have issues\n",
    "#getting this version, try a hard code implementation of the library here - https://pastebin.com/qs1es9XE. There is\n",
    "#no implementation of the labels method for the hard code, but it should be relatively easy to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us read in and process our data for NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad_network_id            3719533\n",
      "ad_type                  3719533\n",
      "advertiser_id            3719533\n",
      "bid_requests             3719533\n",
      "bid_responses            3719533\n",
      "c_cnt                    3719533\n",
      "campaign_id              3719533\n",
      "campaign_type            3719533\n",
      "cr_cnt                   3719533\n",
      "creative_id              3719533\n",
      "f_cnt                    3719533\n",
      "geo_continent_code       3719533\n",
      "geo_country_code2        3719533\n",
      "geo_dma_code             3719533\n",
      "geo_region_name          3719533\n",
      "geo_timezone             3719533\n",
      "i_cnt                    3719533\n",
      "i_timestamp              3719533\n",
      "pub_network_id           3719533\n",
      "r_cnt                    3719533\n",
      "r_num_ads_requested      3719533\n",
      "r_num_ads_returned       3719533\n",
      "r_num_ads_third_party    3719533\n",
      "r_timestamp              3719533\n",
      "rate_metric              3719533\n",
      "session_id               3719533\n",
      "site_id                  3719533\n",
      "token                    3719533\n",
      "ua_device                3719533\n",
      "ua_device_type           3719533\n",
      "ua_name                  3719533\n",
      "ua_os_name               3719533\n",
      "vi_cnt                   3719533\n",
      "vv_cnt                   3719533\n",
      "zone_id                  3719533\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('Day1')\n",
    "\n",
    "#drop rows with c_cnt as NaN\n",
    "df = df[np.isfinite(df['c_cnt'])]\n",
    "\n",
    "#drop columns that have more than 50% NaN values\n",
    "df = df.dropna(thresh=int(0.5*df.shape[0]), axis=1)\n",
    "\n",
    "#drop all samples with any NaN values included\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do some final preprocessing of our dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Turns a timestamp into which minute the time was at - used as a categorical feature.\n",
    "def timestamp_to_min(timestamp, is_hour=True):\n",
    "    if is_hour:\n",
    "        return timestamp.split(':')[0][-2:]\n",
    "    else: \n",
    "        return timestamp.split(':')[1]\n",
    "\n",
    "#plots frequency of a feature's different classes, useful for exploratory analysis\n",
    "def plot_freq(col_name, df):\n",
    "    df_frequency = df.groupby(col_name).agg('count').sort_values('ad_type',ascending=False)\n",
    "    plt.plot([i for i in range(len(df_frequency.values))], [np.log(i[2]) for i in df_frequency.values])\n",
    "    plt.show()\n",
    "\n",
    "#if a feature only has one unique value, it tells us nothing, so we drop it.\n",
    "def remove_only_ones(df):\n",
    "    for col in df.columns:\n",
    "        if len(df[col].unique()) == 1:\n",
    "            df.drop(col, inplace=True,axis=1)\n",
    "\n",
    "#just prints how many unique values are in each feature\n",
    "def print_column_counts(df):    \n",
    "    for i in df:\n",
    "        print(i, df[i].nunique())\n",
    "\n",
    "#We do some final cleaning, changing all non-numerical features into strings for later.\n",
    "def preprocess(df):    \n",
    "    for i in df:\n",
    "        if i[-1] != 't' or i[-2] != 'n' or i[-3] != 'c':\n",
    "            df[i] = df[i].astype('str')\n",
    "    remove_only_ones(df)\n",
    "    if 'site_id' in df.columns:\n",
    "        df.drop('site_id',inplace=True,axis=1)\n",
    "    df['i_timestamp'] = df['i_timestamp'].apply(timestamp_to_min)\n",
    "    df['r_timestamp'] = df['r_timestamp'].apply(timestamp_to_min)\n",
    "    \n",
    "#given a categorical column, we apply our earlier strategy of one-hot-encoding with maximum thresh=200\n",
    "def transform_column(df, col, thresh=200, return_labels=False):\n",
    "    df_frequency = df[[col, 'c_cnt']].groupby(col).agg('count').sort_values('c_cnt',ascending=False)\n",
    "    if df[col].nunique() > thresh:\n",
    "        enc = CategoricalEncoder(categories=[sorted(df_frequency[0:thresh].index.values)],handle_unknown='ignore')\n",
    "        labels = df_frequency[0:thresh].index.values\n",
    "    else:\n",
    "        enc = CategoricalEncoder(categories=[sorted(df_frequency.index.values)],handle_unknown='ignore')\n",
    "        labels = df_frequency.index.values\n",
    "    labels = [str(col) + str(i) for i in labels]\n",
    "    if return_labels:\n",
    "        return labels\n",
    "    enc.fit(df[col].values.reshape(-1, 1))\n",
    "    return enc.transform(df[col].values.reshape(-1,1)).toarray()\n",
    "\n",
    "preprocess(df)\n",
    "#this set contains our numerical column names\n",
    "numerical_features = set(['c_cnt', 'i_cnt', 'r_cnt', 'vi_cnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert our dataframe into X and Y matrices to do machine learning on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''This function takes in the feature matrix, and drops all columns that are closely correlated, up to some threshhold\n",
    "between 0 and 1, with 0 being no correlation and 1 being high correlation. We do this so there is no\n",
    "overrepresentation of any one 'true feature', and to reduce overfitting.'''\n",
    "def remove_corr_features(X, labels, thresh=0.95):\n",
    "    correlation_matrix, corrs, toremove = np.corrcoef(X.T), [], set()\n",
    "    \n",
    "    for i, row in enumerate(correlation_matrix):\n",
    "        for j, corr in enumerate(row):\n",
    "            if j <= i: continue\n",
    "            if corr > thresh:\n",
    "                toremove.add(j)   \n",
    "                corrs.append((corr, labels[i], labels[j]))\n",
    "\n",
    "    X = np.delete(X, [i for i in toremove], 1)\n",
    "    labels = np.delete(labels, [i for i in toremove])\n",
    "    \n",
    "'''\n",
    "This function takes in X as the feature matrix and Y as the label matrix, and undersamples the majority class such that \n",
    "#majority class samples / #minority class samples = pos_ratio. Returns the new X and Y matrices\n",
    "'''\n",
    "def fix_class_imbalance_with_subsampling(X, Y, pos_ratio=9):\n",
    "    Y = Y.reshape(-1,1)\n",
    "    ind_1, ind_0 = [], []\n",
    "    for i, y_h in enumerate(Y):\n",
    "        if y_h: ind_1.append(i)\n",
    "        else: ind_0.append(i)\n",
    "    to_sample = np.random.permutation(pos_ratio*len(ind_1))\n",
    "    to_sample_0 = [ind_0[i] for i in to_sample]\n",
    "    X2 = np.vstack([X[ind_1],X[to_sample_0]])\n",
    "    Y2 = np.vstack([Y[ind_1],Y[to_sample_0]])\n",
    "    Y = Y.reshape(-1)\n",
    "    \n",
    "    new_ind = np.random.permutation(len(X2))\n",
    "    return X2[new_ind],Y2[new_ind]\n",
    "\n",
    "'''\n",
    "This function takes a dataframe with a one hot encoding threshhold and correlation threshhold, and returns\n",
    "X as the feature matrix, Y as the label matrix, and labels as a list of feature names corresponding to X.\n",
    "'''\n",
    "def get_data_matrix(df, one_hot_thresh=5, corr_thresh=0.95):\n",
    "    Y = df['c_cnt'].values\n",
    "    labels = np.hstack([transform_column(df, col, thresh=one_hot_thresh, return_labels=True) if col not in numerical_features else [str(col)]\n",
    "                        for col in df if col != 'c_cnt'])\n",
    "    X = np.hstack([transform_column(df, col, thresh=one_hot_thresh) if col not in numerical_features else df[col].values.reshape(-1,1)\n",
    "               for col in df if col != 'c_cnt'])\n",
    "    remove_corr_features(X, labels, corr_thresh)\n",
    "    return X, Y, labels\n",
    "\n",
    "'''\n",
    "This function does cross validation by splitting our data into training and validation sets.\n",
    "'''\n",
    "def test_train_split(X, Y, thresh=0.7): \n",
    "    #shuffle our data\n",
    "    new_order = np.random.permutation(len(X))\n",
    "    X, Y = X[new_order], Y[new_order]\n",
    "    #split our data\n",
    "    cutoff = int(thresh*len(X))\n",
    "    X_train, Y_train = X[0:cutoff], Y[0:cutoff]\n",
    "    X_test, Y_test = X[cutoff:], Y[cutoff:]\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "'''\n",
    "This function takes in X as the feature matrix and Y as the label matrix, and undersamples the majority class such that \n",
    "#majority class samples / #minority class samples = pos_ratio. Returns the new X and Y matrices. This should only\n",
    "be done on the training dataset - validation CANNOT be undersampled or you introduce outside bias.\n",
    "'''\n",
    "def fix_class_imbalance_with_subsampling(X, Y, pos_ratio=9):\n",
    "    Y = Y.reshape(-1,1)\n",
    "    ind_1, ind_0 = [], []\n",
    "    for i, y_h in enumerate(Y):\n",
    "        if y_h: ind_1.append(i)\n",
    "        else: ind_0.append(i)\n",
    "    to_sample = np.random.permutation(pos_ratio*len(ind_1))\n",
    "    to_sample_0 = [ind_0[i] for i in to_sample]\n",
    "    X2 = np.vstack([X[ind_1],X[to_sample_0]])\n",
    "    Y2 = np.vstack([Y[ind_1],Y[to_sample_0]])\n",
    "    Y = Y.reshape(-1)\n",
    "    \n",
    "    new_ind = np.random.permutation(len(X2))\n",
    "    return X2[new_ind],np.ravel(Y2[new_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual machine learning - we will be focusing on Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We will be using f1 score to measure our models, which is a good performance measurement scalar for datasets\n",
    "where the negatives >> positives.\n",
    "'''\n",
    "def get_f1_score(test_cm):\n",
    "    true_neg  = test_cm[0][0]\n",
    "    false_pos = test_cm[0][1]\n",
    "    false_neg = test_cm[1][0]\n",
    "    true_pos  = test_cm[1][1]\n",
    "    precision = true_pos/(true_pos+false_pos)\n",
    "    recall = true_pos/(true_pos+false_neg)\n",
    "    score = 2*precision*recall/(precision+recall)\n",
    "    return score\n",
    "\n",
    "'''\n",
    "Perform grid search on logistic regression. Our hyperparameters are as below. Returns confusion matrices for each\n",
    "set of hyperparameters.\n",
    "'''\n",
    "def lr_grid_search(df, best, params):\n",
    "    '''\n",
    "    Current best - 0.02941812675751676 (5, 25, 'l2', 0.1, 'newton-cg', 'ovr'), iteration 540\n",
    "    '''\n",
    "    iter = 1\n",
    "    grid_search_dict = {}\n",
    "    Y = df['c_cnt'].values\n",
    "    best = 0\n",
    "\n",
    "    #for one_hot_thresh in [2, 5, 10, 25]:\n",
    "    for one_hot_thresh in [25]:\n",
    "        X = get_data_matrix(df,one_hot_thresh=one_hot_thresh)[0]\n",
    "        for pos_rate in [1, 2, 5, 10, 25, 50]:\n",
    "            X_train, Y_train, X_test, Y_test = test_train_split(X, Y)\n",
    "            X_train, Y_train = fix_class_imbalance_with_subsampling(X_train, Y_train,pos_ratio=pos_rate)\n",
    "\n",
    "            for penalty in ['l1', 'l2']:\n",
    "                for C in [0.01, 0.1, 1]:\n",
    "                    for solver in ['newton-cg', 'sag', 'saga', 'lbfgs']:\n",
    "                        if penalty == 'l1' and solver != 'saga': continue\n",
    "                        for mult_class in ['ovr', 'multinomial']:\n",
    "                            print(iter)\n",
    "                            print(best, params)\n",
    "                            iter += 1\n",
    "                            av_score = 0\n",
    "                            for trial in range(3):\n",
    "                                lr = LogisticRegression(penalty=penalty, C=C, solver=solver, multi_class=mult_class, max_iter=5000)\n",
    "                                lr.fit(X_train, Y_train)\n",
    "                                test_cm = confusion_matrix(Y_test, lr.predict(X_test))\n",
    "                                grid_search_dict[(one_hot_thresh, pos_rate, penalty, C, solver, mult_class)] = test_cm\n",
    "                                av_score += get_f1_score(test_cm)/3\n",
    "                        \n",
    "                            if av_score > best:\n",
    "                                best = av_score\n",
    "                                params = (one_hot_thresh, pos_rate, penalty, C, solver, mult_class)\n",
    "    return grid_search_dict\n",
    "        \n",
    "'''\n",
    "Performs feature scoring by randomly subsampling our features and scoring each subsample. \n",
    "'''\n",
    "def feature_scoring_random_subsample(lst, df, model, oh_thresh, resample_thresh, p=0.3, iterations=10000):\n",
    "    lst, transformed_col, past_score = [], {}, {}\n",
    "    for col in df2.columns:\n",
    "        if col not in numerical_features:\n",
    "            transformed_col[col] = transform_column(df, col, thresh=oh_thresh)\n",
    "    Y = df['c_cnt'].values\n",
    "    scores = []\n",
    "    for i in iterations:\n",
    "        print(\"Iteration: \", j)\n",
    "        #subsample random columns with probability p\n",
    "        selected = [c for c in df2.columns if random.random() < p]\n",
    "        #if we have no items, super unlucky, try again\n",
    "        if len(selected) < 1: continue\n",
    "        X = np.hstack([transformed_col[col] if col not in numerical_features else df[col].values.reshape(-1,1)\n",
    "                   for col in selected])\n",
    "        X_train, Y_train, X_test, Y_test = test_train_split(X, Y)\n",
    "        X_train, Y_train = fix_class_imbalance_with_subsampling(X_train, Y_train,pos_ratio=resample_thresh)\n",
    "\n",
    "        lr = clone(model)\n",
    "        lr.fit(X_train, Y_train)\n",
    "        test_cm = confusion_matrix(Y_test, [bin_round(i) for i in lr.predict(X_test)])\n",
    "        score = get_f1_score(test_cm)\n",
    "        if math.isnan(score): continue\n",
    "        scores.append((selected, score))\n",
    "        \n",
    "        if i > 0 and i % 100 == 0: #Updates score list every 100 iterations, and checks for convergence\n",
    "            feature_scores = {}\n",
    "            for f in df.columns:\n",
    "                feature_score = []\n",
    "                for cols, score in scores:\n",
    "                    if f in cols:\n",
    "                        feature_score.append(score)\n",
    "                feature_scores[f] = np.mean(feature_score)\n",
    "            lst = []\n",
    "            for feat in feature_scores:\n",
    "                lst.append((feature_scores[feat], feat))\n",
    "            lst = sorted(lst)\n",
    "            lst.reverse()\n",
    "            #check for convergence\n",
    "            if i > 100:\n",
    "                has_conv = True\n",
    "                for f in df.columns:\n",
    "                    if abs(past_score[f]-feature_scores[f])/feature_scores[f] > 0.01: has_conv = False\n",
    "                if has_conv:\n",
    "                    return\n",
    "            past_score = feature_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to start using our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best, params = None, None\n",
    "lr_grid_search(df, best, params)\n",
    "print(best, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
